<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Futon note</title><link href="http://daikishimada.github.io/" rel="alternate"></link><link href="http://daikishimada.github.io/feeds/all.atom.xml" rel="self"></link><id>http://daikishimada.github.io/</id><updated>2016-02-19T21:30:00+09:00</updated><entry><title>vimでPythonのコードを書いてると落ちるので対処する</title><link href="http://daikishimada.github.io/vim_python_crash.html" rel="alternate"></link><updated>2016-02-19T21:30:00+09:00</updated><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-02-19:vim_python_crash.html</id><summary type="html">&lt;p&gt;vimでPython書いてると落ちるようになってしまったので，対処した話．&lt;br /&gt;
neocompleteがトリガで．オムニ補完が原因で死んでたらしい．&lt;/p&gt;
&lt;h2 id="vim"&gt;vimが死ぬ&lt;/h2&gt;
&lt;p&gt;Pythonのコードを書いていると，いきなりvimが閉じて，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Vim&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Caught&lt;/span&gt; &lt;span class="n"&gt;deadly&lt;/span&gt; &lt;span class="n"&gt;signal&lt;/span&gt; &lt;span class="n"&gt;TRAP&lt;/span&gt;
&lt;span class="n"&gt;Vim&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;preserving&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;Vim&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Finished&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Trace&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;breakpoint&lt;/span&gt; &lt;span class="n"&gt;trap&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;core&lt;/span&gt; &lt;span class="n"&gt;dumped&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;となって，クラッシュしてしまう．&lt;/p&gt;
&lt;p&gt;タイミング的には，"."を打ち込んだ時だったので，補完で使っているプラグインneocompleteか？&lt;br /&gt;
と思っていた．&lt;/p&gt;
&lt;h2 id="neocomplete"&gt;neocompleteは悪く無い&lt;/h2&gt;
&lt;p&gt;調べてみると，&lt;a href="http://d.hatena.ne.jp/akahana_1/20140617/p2"&gt;neocomplcacheを使っている人で同様の症状がでている人&lt;/a&gt;がいるようだった．&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;neocomplcacheのオムニ補完がこけていたのだという事が判明したので、特定の言語についてオムニ補完を無効にしてみます。
neocomplcacheのドキュメントに書いてあるとおりにオムニ補完を無効。&lt;br /&gt;
&lt;a href="http://d.hatena.ne.jp/akahana_1/20140617/p2"&gt;もしもneocomplcacheでコケてしまった時の解決法 - SEGVな毎日&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;なるほどと思ってneocompleteのドキュメントを読んでみると，&lt;a href="https://github.com/Shougo/neocomplete.vim/blob/master/doc/neocomplete.txt#L1450"&gt;ちゃんと対処法が書いてあった&lt;/a&gt;．&lt;/p&gt;
&lt;p&gt;公式のドキュメントにもあるが，これはオムニ補完のところで死んでいるようなので，&lt;br /&gt;
neocompleteには責任はない．&lt;/p&gt;
&lt;p&gt;Pythonで症状がでていたが，どうやらRubyでも症状が出るようだ．&lt;/p&gt;
&lt;h2 id="_1"&gt;解決策&lt;/h2&gt;
&lt;p&gt;と，いうことで.vimrcにPythonのオムニ補完の無効化を記述する．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;quot; Disable omnifunc in Python
if !exists(&amp;#39;g:neocomplete#sources#omni#input_patterns&amp;#39;)
    let g:neocomplete#sources#omni#input_patterns = {}
endif
let g:neocomplete#sources#omni#input_patterns.python = &amp;#39;&amp;#39;  
&lt;/pre&gt;&lt;/div&gt;</summary><category term="vim"></category><category term="python"></category></entry><entry><title>Feedly + arXiv でやる論文サーベイ</title><link href="http://daikishimada.github.io/feedly_survey.html" rel="alternate"></link><updated>2016-02-18T20:00:00+09:00</updated><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-02-18:feedly_survey.html</id><summary type="html">&lt;p&gt;この前，&lt;a href="http://daikishimada.github.io/wbaflcnn.html"&gt;Convolutional Neural Networksのサーベイを発表した時&lt;/a&gt;，&lt;br /&gt;
論文(コンピュータサイエンス系)の探し方を随分と聞かれたので，まとめてみました．  &lt;/p&gt;
&lt;p&gt;主にFeedlyでarXivみるっていう話ですけど，&lt;br /&gt;
他の情報収集元もあげてみます．&lt;/p&gt;
&lt;h2 id="feedly"&gt;Feedly&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://feedly.com"&gt;Feedly&lt;/a&gt;は，RSSリーダです．&lt;br /&gt;
もちろん，論文サーベイ以外にもRSSリーダとして使えるので，おすすめ．&lt;/p&gt;
&lt;p&gt;（&lt;a href="http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Fdaikishimada.github.io%2Ffeeds%2Fall.atom.xml"&gt;ホームにあるFollowボタンは，Feedlyのこのブログの購読ボタンだったりする&lt;/a&gt;）&lt;/p&gt;
&lt;h2 id="arxiv"&gt;arXiv&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/"&gt;arXiv&lt;/a&gt;（アーカイヴ、archiveと同じ発音）は、物理学、数学、計算機科学、量的生物学、Quantitative Finance, 統計学の、プレプリントを含む様々な論文が保存・公開されているウェブサイトである。論文のアップロード（投稿）、ダウンロード（閲覧）ともに無料で、論文はPDF形式である。1991年にスタートして、プレプリント・サーバーの先駆けとなったウェブサイトである。大文字の X をギリシャ文字のカイ（Χ）にかけて archive と読ませている。
&lt;a href="https://ja.wikipedia.org/wiki/ArXiv"&gt;arXiv - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;私がよくチェックする機械学習やコンピュータビジョン界隈では，arXivの存在はかなり知られていて，大きな国際会議に投稿されたものがアップロードされていたりします．&lt;br /&gt;
webに論文をあげるので，後から修正や補足が出来る様になっていることも，良いポイントだと思います．&lt;/p&gt;
&lt;h2 id="feedlyarxiv"&gt;FeedlyでarXivをトラッキングする&lt;/h2&gt;
&lt;p&gt;簡単にFeedlyでarXivの好きなサブジェクトを追いかける方法をまとめます．&lt;br /&gt;
（あらかじめFeedlyの会員登録がされている体で話を進めます）&lt;/p&gt;
&lt;p&gt;例として，&lt;a href="http://arxiv.org/corr/home"&gt;Computing Research Repository&lt;/a&gt;の&lt;a href="http://arxiv.org/list/cs.AI/recent"&gt;Artificial Intelligence (cs.AI)&lt;/a&gt;をFeedlyに登録してみます．&lt;br /&gt;
（CoRRのサブジェクトは全部で&lt;a href="http://arxiv.org/corr/subjectclasses"&gt;こんなに&lt;/a&gt;ある）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;arXivにアクセス&lt;br /&gt;
&lt;img alt="arXivにアクセス" src="https://www.evernote.com/l/AQCtPOQuqwRBLbFghoivStlyfAIGojywMbwB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/corr/home"&gt;Computing Research Repository&lt;/a&gt;の&lt;a href="http://arxiv.org/list/cs.AI/recent"&gt;Artificial Intelligence (cs.AI)&lt;/a&gt;へアクセス&lt;br /&gt;
&lt;img alt="csAIへアクセス" src="https://www.evernote.com/l/AQBo0xAaPi1MJZNLAeLr5gPGwZaM6ny6A8oB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;URLをコピーして置く&lt;br /&gt;
&lt;img alt="URLをコピーして置く" src="https://www.evernote.com/l/AQAePkFajEJCKKP45CYFEolVWZNbT-L5EI8B/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feedlyを開いて Add Contentをクリック&lt;br /&gt;
&lt;img alt="FeedlyのAddContentをクリック" src="https://www.evernote.com/l/AQB7CvAm-rBI4pGAEn2QlWIWhFgdHg2GMj0B/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;検索窓に購読したいページのURLを入力&lt;br /&gt;
&lt;img alt="検索窓に購読したいページのURLを入力" src="https://www.evernote.com/l/AQAecTpbRY9AeJw5dqa6TYFvxr6Mier_K0oB/image.png" /&gt;&lt;br /&gt;
&lt;img alt="入力ごEnter" src="https://www.evernote.com/l/AQANGZ23_GlH1LDX3OA7RazlEkyeet2OvTcB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;+を押してFeedlyへの登録画面へ
&lt;img alt="+を押してFeedlyへの登録画面へ" src="https://www.evernote.com/l/AQBuUm7gaL5GF7Lo6EGKmTy6TT7Tq_v9E0EB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;追加するコレクションを選んでAddを推す
&lt;img alt="追加するコレクションを選んでAddを推す" src="https://www.evernote.com/l/AQCjCjJ9cMVIPZNTSQhhFQrT47iy8ZgI8yIB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これでFeedlyで論文がトラックできるようになっているはず．&lt;br /&gt;
他のサブジェクトを登録したい場合はさっきまでの作業を繰り返せばOKです．&lt;/p&gt;
&lt;p&gt;こんな感じ．
もし，"by ~"の左に数字が出ているときはそれは「&lt;a href="https://feedly.uservoice.com/knowledgebase/articles/178821-what-does-the-number-of-green-hearts-mean"&gt;engagement&lt;/a&gt;」を示す数字です．&lt;br /&gt;
その論文がどれくらいバズっているかわかるので，私がFeedlyを使う理由の一つでもあったりします．
&lt;img alt="こんな感じ" src="https://www.evernote.com/l/AQAwGNBx0oVHWrU_v4-eQ2LuMCDmXP38E7EB/image.png" /&gt;&lt;/p&gt;
&lt;p&gt;記事をクリックすれば，アブストラクトも読める！&lt;br /&gt;
&lt;img alt="記事をクリックすればアブストラクトも読める" src="https://www.evernote.com/l/AQCXJIngh0lN5KqsxuGDjGuzYhXtYbzVa9gB/image.png" /&gt;&lt;/p&gt;
&lt;p&gt;"Visit Website" をクリックすれば，arXivの該当ページに飛んで，pdfやbibtexのファイルを入手出来ます．やったね！
&lt;img alt="VisitWebsite" src="https://www.evernote.com/l/AQBd3Vae5WhKRK9t0TTr4rVGNwDa5VHMZp0B/image.png" /&gt;&lt;/p&gt;
&lt;h2 id="reddit"&gt;Redditで技術系のスレッドを見つける&lt;/h2&gt;
&lt;p&gt;arXivの話だけだと寂しいので，他の情報収集元を言うと，&lt;br /&gt;
Redditの機械学習系の議論はたまに目を通します．&lt;/p&gt;
&lt;p&gt;なかなか面白い議論のときもあれば，過疎ってるときもあるけど…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;reddit（レディット）は英語圏のウェブサイト。ニュース記事、画像のリンクやテキストを投稿し、コメントをつけることが可能。ウェブサイトへのリンクを収集・公開するソーシャルブックマークサイト。ニュース記事、画像などの紹介や感想募集のトピックを誰でも立てられるソーシャルニュースサイトでもあり、トピックについてのコメントを誰でも書き込める電子掲示板の一種でもある。 &lt;a href="https://ja.wikipedia.org/wiki/Reddit"&gt;reddit - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://twitter.com/mxlearn"&gt;ML@REDDIT&lt;/a&gt;をフォローすると，&lt;br /&gt;
機械学習系のスレッドがどんどん流れてくるので，チェックしています．&lt;/p&gt;</summary><category term="paper"></category><category term="研究"></category><category term="web"></category><category term="AI"></category></entry><entry><title>Deep Learning のトレンドについて喋ってきた</title><link href="http://daikishimada.github.io/wbaflcnn.html" rel="alternate"></link><updated>2016-02-18T17:00:00+09:00</updated><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-02-18:wbaflcnn.html</id><summary type="html">&lt;p&gt;Convolutional Neural NetworksのトレンドについてCasualじゃない話をしてきました．&lt;/p&gt;
&lt;h2 id="_1"&gt;全脳アーキテクチャ若手の会カジュアルトーク&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://wbawakate.jp/posts/events/12th/"&gt;全脳アーキテクチャ若手の会カジュアルトーク&lt;/a&gt;
というところでお話をしてきました．&lt;br /&gt;
ちょっと層がわからなかったのですが，IT系のエンジニアの方が多かったみたいです．&lt;br /&gt;
（学生は4人くらい…？しかもほぼ身内）&lt;/p&gt;
&lt;p&gt;僕のスライドはSlide Shareの方にアップロードされています．&lt;br /&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/NIXhrnmgjyU0xK" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="//www.slideshare.net/sheemap/convolutional-neural-networks-wbafl2" title="Convolutional Neural Networks のトレンド @WBAFLカジュアルトーク#2" target="_blank"&gt;Convolutional Neural Networks のトレンド @WBAFLカジュアルトーク#2&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a target="_blank" href="//www.slideshare.net/sheemap"&gt;Daiki Shimada&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;しかも，映像もアップロードされていた…  &lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/cBog5o9ebcQ" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2 id="_2"&gt;発表後記&lt;/h2&gt;
&lt;p&gt;実際はConvolutional Neural Networks(CNN)系論文128本ノックにするつもりだったとはいえない空気でしたね…&lt;/p&gt;
&lt;p&gt;個人的には画像生成やキャプション生成系の研究速度はとても速く進んでいると感じています．  &lt;br /&gt;
Visual Turing Test の話はもう少し掘り下げたかったですね．    &lt;/p&gt;
&lt;p&gt;Deep Mind が DQNで３D一人称視点ゲームを遊ぶ話がちょうど発表直前に出ていて，alphaGoの話題とともに紹介しましたが，あの領域は完全にGoogleの独壇場かなと思います．&lt;/p&gt;
&lt;p&gt;本当にここ最近はすごいペースで研究が進んでいくので，どこかで一回整理しないといけないなっていう危機感があり，自分なりにまとめました．  &lt;br /&gt;
サーベイってけっこう上手くやらないと労力使うし，大事な研究を見落とすこともあるので，こういう資料がだれかのサーベイの種なればいいなあ，なんて．  &lt;br /&gt;
（各々，独自にまとめていって，こういう資料がどんどん増えることを願う）&lt;/p&gt;
&lt;h2 id="_3"&gt;紹介文献一覧&lt;/h2&gt;
&lt;h3 id="cnn"&gt;CNNアーキテクチャの変遷 / 最適化手法&lt;/h3&gt;
&lt;h4 id="cnn_1"&gt;CNNアーキテクチャ&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Neocognitron&lt;ul&gt;
&lt;li&gt;K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics 36, 1980.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LeNet&lt;ul&gt;
&lt;li&gt;Y LeCun, L Bottou, Y Bengio, P Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 1998.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ave./Max Pooling, Local Contrast Normalization&lt;ul&gt;
&lt;li&gt;K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun. What is the best multi-stage architecture for object recognition?. CVPR, 2009.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ReLU&lt;ul&gt;
&lt;li&gt;X. Glorot, A. Bordes, Y. Bengio. Deep Sparse Rectifier Neural Networks. AISTATS 11, 2011.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dropout&lt;ul&gt;
&lt;li&gt;G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov. &lt;a href="http://arxiv.org/abs/1207.0580"&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt;. arXiv: 1207.0580, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AlexNet&lt;ul&gt;
&lt;li&gt;A. Krizhevsky, I. Sutskever, G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. NIPS, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Network in Network, global ave. pooling&lt;ul&gt;
&lt;li&gt;M. Lin, Q. Chen, S. Yan. &lt;a href="http://arxiv.org/abs/1312.4400"&gt;Network In Network&lt;/a&gt;. arXiv: 1312.4400, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VGG-Net&lt;ul&gt;
&lt;li&gt;K. Simonyan, A. Zisserman. &lt;a href="http://arxiv.org/abs/1409.1556"&gt;Very Deep Convolutional Networks for Large-Scale Visual Recognition&lt;/a&gt;. arXiv: 1409.1556, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GoogLeNet / Inception&lt;ul&gt;
&lt;li&gt;C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich. &lt;a href="http://arxiv.org/abs/1409.4842"&gt;Going deeper with convolutions&lt;/a&gt;. arXiv: 1409.4842, 2014.&lt;/li&gt;
&lt;li&gt;C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna. &lt;a href="http://arxiv.org/abs/1512.00567"&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;. arXiv: 1512.00567, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SPP-Net&lt;ul&gt;
&lt;li&gt;K. He, X. Zhang, S. Ren, J. Sun. &lt;a href="http://arxiv.org/abs/1406.4729"&gt;Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;. arXiv: 1406.4729, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All Convolutional Net, guided BackPropagation&lt;ul&gt;
&lt;li&gt;J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller. &lt;a href="http://arxiv.org/abs/1412.6806"&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt;. arXiv: 1412.6806, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exemplar CNN&lt;ul&gt;
&lt;li&gt;A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, T. Brox. &lt;a href="http://arxiv.org/abs/1406.6909"&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;. arXiv: 1406.6909, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Triplet network&lt;ul&gt;
&lt;li&gt;E. Hoffer, N. Ailon. &lt;a href="http://arxiv.org/abs/1412.6622"&gt;Deep metric learning using Triplet network&lt;/a&gt;. arXiv: 1412.6622, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Batch Normalization&lt;ul&gt;
&lt;li&gt;S. Ioffe, C. Szegedy. &lt;a href="http://arxiv.org/abs/1502.03167"&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;. arXiv: 1502.03167, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Residual Network; ResNet&lt;ul&gt;
&lt;li&gt;K. He, X. Zhang, S. Ren, J. Sun. &lt;a href="http://arxiv.org/abs/1512.03385"&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;. arXiv: 1512.03385, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="_4"&gt;確率的勾配降下法における学習率調整法&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AdaGrad&lt;ul&gt;
&lt;li&gt;J. Duchi, E. Hazan, Y. Singer. &lt;a href="http://stanford.edu/~jduchi/projects/DuchiHaSi10_colt.pdf"&gt;Adaptive Subgradient Methods for Online Learning and Stochastic Optimization&lt;/a&gt;. Journal of Machine Learning Research 12 ,2011.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RMSProp&lt;ul&gt;
&lt;li&gt;T. Tieleman, G. Hinton. Divide the gradient by a running average of its recent magnitude. &lt;a href="https://www.coursera.org/course/neuralnets"&gt;COURSERA: Neural Networks for Machine Learning 4&lt;/a&gt;, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AdaDelta&lt;ul&gt;
&lt;li&gt;M. D. Zeiler. &lt;a href="http://arxiv.org/abs/1212.5701"&gt;ADADELTA: An Adaptive Learning Rate Method&lt;/a&gt;. arXiv: 1212.5701, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adam&lt;ul&gt;
&lt;li&gt;D. Kingma, J. Ba. &lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. arXiv: 1412.6980, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_5"&gt;特徴量の解析 / 可視化&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeconvNet for visualizing&lt;ul&gt;
&lt;li&gt;M.D. Zeiler, and R. Fergus. &lt;a href="http://arxiv.org/abs/1311.2901"&gt;Visualizing and understanding convolutional networks&lt;/a&gt;. arXiv,: 1311.2901, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;入力画像の最適化&lt;ul&gt;
&lt;li&gt;A. Mahendran, A. Vedaldi. &lt;a href="http://arxiv.org/abs/1412.0035"&gt;Understanding Deep Image Representations by Inverting Them&lt;/a&gt;. arXiv: 1412.0035, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CNNをだます&lt;ul&gt;
&lt;li&gt;C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, R. Fergus. &lt;a href="http://arxiv.org/abs/1312.6199"&gt;Intriguing properties of neural networks&lt;/a&gt;. arXiv: 1312.6199, 2013.&lt;/li&gt;
&lt;li&gt;I. J. Goodfellow, J. Shlens, C. Szegedy. &lt;a href="http://arxiv.org/abs/1412.6572"&gt;Explaining and Harnessing Adversarial Examples&lt;/a&gt;. arXiv: 1412.6572, 2014.&lt;/li&gt;
&lt;li&gt;A. Nguyen, J. Yosinski, J. Clune. &lt;a href="http://arxiv.org/abs/1412.1897"&gt;Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images&lt;/a&gt;. arXiv: 1412.1897, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_6"&gt;物体検出・領域分割&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;R-CNN&lt;ul&gt;
&lt;li&gt;R. Girshick, J. Donahue, T. Darrell, J. Malik. &lt;a href="http://arxiv.org/abs/1311.2524"&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;. arXiv:1311.2524, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fast R-CNN&lt;ul&gt;
&lt;li&gt;R. Girshick. &lt;a href="http://arxiv.org/abs/1504.08083"&gt;Fast R-CNN&lt;/a&gt;. arXiv:1504.08083, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Faster R-CNN&lt;ul&gt;
&lt;li&gt;S. Ren, K. He, R. Girshick, J. Sun. &lt;a href="http://arxiv.org/abs/1506.01497"&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;. arXiv:1506.01497, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fully Convolutional Networks (FCN)&lt;ul&gt;
&lt;li&gt;K. Simonyan, A. Vedaldi, A. Zisserman. &lt;a href="http://arxiv.org/abs/1312.6034"&gt;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps&lt;/a&gt;. arXiv: 1312.6034, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SegNet&lt;ul&gt;
&lt;li&gt;V. Badrinarayanan, A. Handa, R. Cipolla. &lt;a href="http://arxiv.org/abs/1505.07293"&gt;SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling&lt;/a&gt;. arXiv: 1505.07293, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CNN + 条件付き確率場(CRF)&lt;ul&gt;
&lt;li&gt;] S. Zheng, S. Jayasumana, B. R. Paredes, V. Vineet, Z. Su, D. Du, C. Huang, P. H. S. Torr. &lt;a href="http://arxiv.org/abs/1502.03240"&gt;Conditional Random Fields as Recurrent Neural Networks&lt;/a&gt;. arXiv: 1502.03240, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep Mask&lt;ul&gt;
&lt;li&gt;P. O. Pinheiro, R. Collobert, P. Dollar. &lt;a href="http://arxiv.org/abs/1506.06204"&gt;Learning to Segment Object Candidates&lt;/a&gt;. arXiv: 1506.06204, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep Face&lt;ul&gt;
&lt;li&gt;Y. Taigman, M. Yang, M. A. Ranzato and L. Wolf. DeepFace: Closing the Gap to Human-Level Performance in Face Verification. CVPR, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spatial Transformer Networks&lt;ul&gt;
&lt;li&gt;M. Jaderberg, K. Simonyan, A. Zisserman, K. Kavukcuoglu. &lt;a href="http://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Networks&lt;/a&gt;. arXiv: 1506.02025, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_7"&gt;画像生成・超解像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep Dream&lt;ul&gt;
&lt;li&gt;&lt;a href="http://googleresearch.blogspot.jp/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Inceptionism: Going Deeper into Neural Networks&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;K. Simonyan, A. Vedaldi, A. Zisserman. &lt;a href="http://arxiv.org/abs/1312.6034"&gt;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps&lt;/a&gt;. arXiv: 1312.6034, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モーフィング&lt;ul&gt;
&lt;li&gt;A. Dosovitskiy, J. T. Springenberg, M. Tatarchenko, T. Brox. &lt;a href="http://arxiv.org/abs/1411.5928"&gt;Learning to Generate Chairs, Tables and Cars with Convolutional Networks&lt;/a&gt;. arXiv: 1411.5928, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画風変換&lt;ul&gt;
&lt;li&gt;L. A. Gatys, A. S. Ecker, M. Bethge. &lt;a href="http://arxiv.org/abs/1508.06576"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;. arXiv: 1508.06576, 2015.&lt;/li&gt;
&lt;li&gt;C. Li, M. Wand. &lt;a href="http://arxiv.org/abs/1601.04589"&gt;Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis&lt;/a&gt;. arXiv:1601.04589, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像生成とベクトル演算性&lt;ul&gt;
&lt;li&gt;A. Radford, L. Metz, S. Chintala. &lt;a href="http://arxiv.org/abs/1511.06434"&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt;. arXiv:1511.06434, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;超解像&lt;ul&gt;
&lt;li&gt;C. Dong, C. C. Loy, K. He, X. Tang. &lt;a href="http://arxiv.org/abs/1501.00092"&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/a&gt;. arXiv:1501.00092, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://waifu2x.udp.jp/index.ja.html"&gt;waifu2x&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deblurring (モーションブラー除去)&lt;ul&gt;
&lt;li&gt;J. Sun, W. Cao, Z. Xu, J. Ponce. &lt;a href="http://arxiv.org/abs/1503.00593"&gt;Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal&lt;/a&gt;. arXiv:1503.00593, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自動彩色&lt;ul&gt;
&lt;li&gt;&lt;a href="http://tinyclouds.org/colorize/"&gt;Automatic Colorization&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;B. Hariharan, P. Arbeláez, R. Girshick, J. Malik. &lt;a href="http://arxiv.org/abs/1411.5752"&gt;Hypercolumns for Object Segmentation and Fine-grained Localization&lt;/a&gt;. arXiv: 1411.5752, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3d"&gt;3Dタスクへ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep Stereo&lt;ul&gt;
&lt;li&gt;J. Flynn, I. Neulander, J. Philbin, N. Snavely. &lt;a href="http://arxiv.org/abs/1506.06825"&gt;DeepStereo: Learning to Predict New Views from the World's Imagery&lt;/a&gt;. arXiv:1506.06825, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=cizgVZ8rjKA"&gt;DeepStereo: Learning to Predict New Views from the World’s Imagery - YouTube&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ステレオマッチング&lt;ul&gt;
&lt;li&gt;J. Žbontar, Y. LeCun. &lt;a href="http://arxiv.org/abs/1510.05970"&gt;Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches&lt;/a&gt;. arXiv: 1510.05970, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;単一画像による3Dタスク&lt;ul&gt;
&lt;li&gt;D. Eigen, R. Fergus. &lt;a href="http://arxiv.org/abs/1411.4734"&gt;Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture&lt;/a&gt;. arXiv: 1411.4734, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_8"&gt;映像への挑戦&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;スポーツ映像分類&lt;ul&gt;
&lt;li&gt;A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, F. Li. Large-scale Video Classification with Convolutional Neural Networks. CVPR, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_9"&gt;より “人間らしい” 機械知覚へ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MemNet: CNN for Memorability&lt;ul&gt;
&lt;li&gt;&lt;a href="http://memorability.csail.mit.edu/index.html"&gt;LaMem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A. Khosla, A. S. Raju, A. Torralba and A. Oliva. Understanding and Predicting Image Memorability at a Large Scale. ICCV, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_10"&gt;マルチモーダル・アプリケーション&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;キャプション生成&lt;ul&gt;
&lt;li&gt;O. Vinyals, A. Toshev, S. Bengio, D. Erhan. Show and Tell: A Neural Image Caption Generator. arXiv: 1411.4555, 2014.&lt;/li&gt;
&lt;li&gt;J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell. &lt;a href="http://arxiv.org/abs/1411.4389"&gt;Long-term Recurrent Convolutional Networks for Visual Recognition and Description&lt;/a&gt;. arXiv: 1411.4389, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像に関する質問文に答える&lt;ul&gt;
&lt;li&gt;H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, W. Xu. &lt;a href="http://arxiv.org/abs/1505.05612"&gt;Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering&lt;/a&gt;. arXiv: 1505.05612, 2015.&lt;/li&gt;
&lt;li&gt;M. Malinowski, M. Rohrbach, M. Fritz. Ask Your Neurons: A Neural-Based Approach to Answering Questions About Images. ICCV, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;マルチモーダルな情報表現&lt;ul&gt;
&lt;li&gt;R. Kiros, R. Salakhutdinov, R. S. Zemel. &lt;a href="http://arxiv.org/abs/1411.2539"&gt;Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models&lt;/a&gt;. arXiv: 1411.2539, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cnn_2"&gt;CNNと強化学習&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Playing Atari&lt;ul&gt;
&lt;li&gt;V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller. &lt;a href="http://arxiv.org/abs/1312.5602"&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt;. arXiv:1312.5602, 2013.&lt;/li&gt;
&lt;li&gt;V. Mnih, at al. Human-level control through deep reinforcement learning. nature, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AlphaGo&lt;ul&gt;
&lt;li&gt;D. Silver, et al. Mastering the game of Go with deep neural networks and tree search. nature, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Facebookによる囲碁AI&lt;ul&gt;
&lt;li&gt;Y. Tian, Y. Zhu. &lt;a href="http://arxiv.org/abs/1511.06410"&gt;Better Computer Go Player with Neural Network and Long-term Prediction&lt;/a&gt;. arXiv: 1511.06410, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一人称3Dゲーム&lt;ul&gt;
&lt;li&gt;V. Mnih, A.P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu. &lt;a href="http://arxiv.org/abs/1602.01783"&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;. arXiv:1602.01783, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="whats-next"&gt;What’s Next ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Visual Genome&lt;ul&gt;
&lt;li&gt;&lt;a href="https://visualgenome.org/"&gt;Visual Genome&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="deeplearning"></category></entry><entry><title>Pelican + Github Pages でブログを作った話</title><link href="http://daikishimada.github.io/pelican-start.html" rel="alternate"></link><updated>2016-01-05T00:50:00+09:00</updated><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-01-05:pelican-start.html</id><summary type="html">&lt;p&gt;Pelicanでこのサイトを作った時のメモ．&lt;/p&gt;
&lt;h2 id="pelican"&gt;Pelicanとは？&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/getpelican/pelican/tree/3.4.0"&gt;getpelican/pelican at 3.4.0&lt;/a&gt;は，Pythonで作られた静的webサイトジェネレータです．&lt;/p&gt;
&lt;p&gt;フランス語の"notebook"を意味する"calepin"から名付けられたらしい．&lt;br /&gt;
reStructuredText か Markdownでページ内容を記述できるのが魅力的です．&lt;/p&gt;
&lt;p&gt;テーマはWordpressほどじゃないですが，それなりにあります．&lt;br /&gt;
&lt;a href="http://www.pelicanthemes.com/"&gt;Pelican themes&lt;/a&gt;から好きなものを選ぶ事ができます．&lt;/p&gt;
&lt;p&gt;Pythonユーザとして，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;やる気出せばいじれないこともない&lt;/li&gt;
&lt;li&gt;テーマがそこそこカッコいい&lt;/li&gt;
&lt;li&gt;Markdownで記事が書ける&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.getpelican.com/en/3.6.3/"&gt;ドキュメント&lt;/a&gt;が充実している
という理由で採用しました．&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pelican_1"&gt;Pelicanの導入&lt;/h2&gt;
&lt;p&gt;pipを使えばあっという間．Markdownも入れておきます．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;pip install pelican Markdown
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;インタラクティブなコマンドツールで初期設定を済ましてくれます．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;mkdir ./blog
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd &lt;/span&gt;blog
&lt;span class="nv"&gt;$ &lt;/span&gt;pelican-quickstart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;以下のような形で質問に回答しました．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Welcome to pelican-quickstart v3.6.3.

This script will help you create a new Pelican-based website.

Please answer the following questions so this script can generate the files
needed by Pelican.


&amp;gt; Where do you want to create your new web site? [.] .
&amp;gt; &amp;gt; What will be the title of this web site? Futon note
&amp;gt; &amp;gt; Who will be the author of this web site? DaikiShimada
&amp;gt; &amp;gt; What will be the default language of this web site? [en] ja
&amp;gt; &amp;gt; Do you want to specify a URL prefix? e.g., http://example.com   (Y/n) y
&amp;gt; &amp;gt; What is your URL prefix? (see above example; no trailing slash) daikishimada.github.io
&amp;gt; &amp;gt; Do you want to enable article pagination? (Y/n) y
&amp;gt; &amp;gt; How many articles per page do you want? [10] 5
&amp;gt; &amp;gt; What is your time zone? [Europe/Paris] Asia/Tokyo
&amp;gt; &amp;gt; Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) y
&amp;gt; &amp;gt; Do you want an auto-reload &amp;amp; simpleHTTP script to assist with theme and site development? (Y/n) n
&amp;gt; &amp;gt; Do you want to upload your website using FTP? (y/N) n
&amp;gt; &amp;gt; Do you want to upload your website using SSH? (y/N) n
&amp;gt; &amp;gt; Do you want to upload your website using Dropbox? (y/N) n
&amp;gt; &amp;gt; Do you want to upload your website using S3? (y/N) n
&amp;gt; &amp;gt; Do you want to upload your website using Rackspace Cloud Files? (y/N) n
&amp;gt; &amp;gt; Do you want to upload your website using GitHub Pages? (y/N) y
&amp;gt; &amp;gt; Is this your personal page (username.github.io)? (y/N) y
&amp;gt; Done. Your new project is available at /Users/sheema/Documents/web/pelican-test
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;ここで，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;gt; &amp;gt; Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) y
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;と答えておくと，ディレクトリにHTML生成用のMakefileとfabfile.pyを生成してくれます．&lt;br /&gt;
makeコマンドやfabコマンドでHTMLを生成出来るようになるので，pelicanのオプションを毎回叩く必要がなくなる（はず）．&lt;/p&gt;
&lt;p&gt;あとは，&lt;code&gt;content/&lt;/code&gt;以下に記事を書いていけばok.  &lt;/p&gt;
&lt;p&gt;ちなみに，ブログじゃないページは&lt;code&gt;content/pages/&lt;/code&gt;にファイルを置いていけば良いみたいです．&lt;/p&gt;
&lt;h3 id="_1"&gt;記事の生成と確認&lt;/h3&gt;
&lt;p&gt;Markdownからhtmlに書き出して欲しい時は，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;make html
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;でhtmlを吐き出してくれます．初期設定では&lt;code&gt;output/&lt;/code&gt;以下に内容が出力されます．&lt;/p&gt;
&lt;p&gt;htmlの確認をする時は，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;make serve
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;とすると，http://localhost:8000でwebサーバを走らせてくれるので，表示などをローカル環境で確認できます．&lt;/p&gt;
&lt;p&gt;公開用にhtmlを吐き出したい時は，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;make publish
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;でやってくれます．&lt;/p&gt;
&lt;h2 id="_2"&gt;テーマの導入&lt;/h2&gt;
&lt;p&gt;ロシア製のテーマ&lt;a href="https://github.com/Samael500/w3-personal-blog"&gt;Samael500/w3-personal-blog&lt;/a&gt;をお借りします．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;git clone https://github.com/Samael500/w3-personal-blog.git
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;ところどころ文言がロシア語なので，&lt;code&gt;w3-personal-blog/template/&lt;/code&gt;内のファイルの一部を適宜書き換える．&lt;br /&gt;
ここが地味に大変だった…&lt;/p&gt;
&lt;h3 id="pelican_2"&gt;pelican にテーマを反映&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pelicanconf.py&lt;/code&gt;にはサイトの設定を書き込む様になっています．&lt;br /&gt;
以下の記述をして，pelicanに使うテーマのディレクトリを知らせる．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;THEME = &amp;#39;./w3-personal-blog-master&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="github-pages"&gt;Github Pages のリポジトリの準備&lt;/h2&gt;
&lt;p&gt;Github Pagesにはユーザが1つだけ持てるサイトとリポジトリ毎に作れるサイト(Project page)の2種類があります．&lt;br /&gt;
今回はユーザ用のサイトを作成しました．&lt;/p&gt;
&lt;h3 id="_3"&gt;公開用リポジトリの作成&lt;/h3&gt;
&lt;p&gt;ユーザ用サイトは&lt;code&gt;ユーザ名.github.io&lt;/code&gt;というリポジトリを作成すれば良いので，&lt;code&gt;daikihsimada.github.io&lt;/code&gt;というリポジトリを作成．&lt;/p&gt;
&lt;p&gt;このリポジトリのmasterブランチにhtmlファイルをアップロードすると，自動的に&lt;code&gt;ユーザ名.github,io&lt;/code&gt;というURLのwebサイトとしてアクセスができるようになります．&lt;/p&gt;
&lt;h3 id="_4"&gt;ソース管理用リポジトリの作成&lt;/h3&gt;
&lt;p&gt;webサイトの公開自体は&lt;code&gt;ユーザ名.github.io&lt;/code&gt;で行いますが，サイト生成に必要なMarkdownファイルやテーマ，pelicanの諸々のファイルは公開する必要が無いので，別のprivateなリポジトリで管理することにしました．&lt;/p&gt;
&lt;p&gt;pelicanをセットアップした&lt;code&gt;blog/&lt;/code&gt;を管理用リポジトリで管理することにしました．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;git init
git remote add origin 管理用リポジトリ
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;それと，.gitignoreは以下のように．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;*.pyc
output/
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="_5"&gt;サイトの公開&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/davisp/ghp-import"&gt;davisp/ghp-import&lt;/a&gt;というPythonモジュールを利用すれば，便利にGithub Pagesへのページ公開が行えます．&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip install ghp-import
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;適当に記事を書いて，公開してみる.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;make publish

&lt;span class="c"&gt;# outputディレクトリをgh-pagesブランチに&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;ghp-import output

&lt;span class="c"&gt;# gh-pagesブランチを公開リポジトリのmasterへpush&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;git push https://github.com/DaikiShimada/daikishimada.github.io.git gh-pages:master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;テーマのカスタマイズにこだわると，時間はかかってしまいますが，
比較的簡単にサイトを公開できます．&lt;/p&gt;
&lt;p&gt;テーマのカスタマイズも，Pythonユーザなら比較的わかりやすい記述になっているので，少しコードを読めば大丈夫だと思います．&lt;br /&gt;
vim で記事を書いてGitで記事のソースまで管理できてしまうので，精神衛生上すごく快適です．&lt;/p&gt;
&lt;h2 id="_6"&gt;参考文献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://memo.laughk.org/2014/08/10/tinker2pelican-repo.html"&gt;ブログジェネレータをTinkererからPelicanに移行した - 続・ラフなラボ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://draftcode.github.io/2012/03/16/42a4fd48-6f58-11e1-bdce-040ccee352e6.html"&gt;PelicanとGitHub Pagesを使う&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dackdive.hateblo.jp/entry/2015/12/20/124055"&gt;[Python]PelicanでGitHub Pages上にブログを構築する(Django Girls for Everyone #2 参加メモ) - dackdive's blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.sotm.jp/2014/01/04/Pelican-Markdown-GithubPages-install-guide/#github-pages_1"&gt;Pelican + Markdown + GitHub Pagesで管理するブログの作り方 - blog@sotm.jp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="pelican"></category><category term="web"></category><category term="python"></category></entry><entry><title>初投稿</title><link href="http://daikishimada.github.io/first-post.html" rel="alternate"></link><updated>2016-01-02T04:00:00+09:00</updated><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-01-02:first-post.html</id><summary type="html">&lt;p&gt;なんかもう，ブログを作るのは3つ目くらいなんだけれども，
はてなも微妙にいじりづらい，Wordpressも管理とかプラグインとか辛いというわがままの結果，
"Github pagesでホスティングをして，Markdownから静的にhtmlを吐いて"ブログを書くことにしてみました．&lt;/p&gt;
&lt;p&gt;これなら，Gitで記事の管理もできるし(前は自分でGit経由でWPをポストするスクリプトを書いていた!!)，
プロフィールのような記事じゃないページも好きな様に作ることが出来る．めでたし．&lt;/p&gt;
&lt;h2 id="_1"&gt;知っていることを吐き出すということ&lt;/h2&gt;
&lt;p&gt;去年は，何かやった記録やノウハウは全てEvernoteへ詰め込んでいて，
別に外に見える形で記録をしなくても良いんじゃないかと思っていました．&lt;/p&gt;
&lt;p&gt;しかし，他の方から何かを尋ねられることもあったり，
自分でも"こういう文章があると色んな人が時間を無駄にせずすむよな"と思うこともあったりで，
ちょっとしたことでも外からアクセスできる形で残しておくことにしました．&lt;/p&gt;
&lt;h2 id="_2"&gt;書くこと&lt;/h2&gt;
&lt;p&gt;当面は，時間があるときに研究で調べたことや，自分で興味があることについて備忘録的にまとめてみよう...&lt;/p&gt;
&lt;p&gt;こういう文章がほしいなと思うものもあるので，
少しずつ少しずつ書いては公開してみるものもあるかな...
(特に，汎用人工知能&lt;AGI&gt;やConvolutional Neural Networksについては私の専門が近いながら体系的な文章が少ないので，少しずつ文章にしてみたい)&lt;/p&gt;
&lt;p&gt;そういえば，これは数式が書けるのか？
それとソースコードのハイライトはどんな感じで写るんだ...確認していなかった...&lt;/p&gt;
&lt;h2 id="_3"&gt;サイト周りでやるべきこと&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数式(LaTex)を表示できるようにする&lt;/li&gt;
&lt;li&gt;プロフィールページを作る(ja/en)&lt;/li&gt;
&lt;li&gt;pelican(このサイトのジェネレータ)について少し書く&lt;/li&gt;
&lt;li&gt;DUSQUSを入れてみる(使う人いるのか？)&lt;/li&gt;
&lt;/ul&gt;</summary><category term="雑記"></category><category term="AI"></category><category term="研究"></category></entry></feed>