<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Futon note</title><link href="http://daikishimada.github.io/" rel="alternate"></link><link href="http://daikishimada.github.io/feeds/research.atom.xml" rel="self"></link><id>http://daikishimada.github.io/</id><updated>2016-02-18T20:00:00+09:00</updated><entry><title>Feedly + arXiv でやる論文サーベイ</title><link href="http://daikishimada.github.io/feedly_survey.html" rel="alternate"></link><published>2016-02-18T20:00:00+09:00</published><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-02-18:feedly_survey.html</id><summary type="html">&lt;p&gt;この前，&lt;a href="http://daikishimada.github.io/wbaflcnn.html"&gt;Convolutional Neural Networksのサーベイを発表した時&lt;/a&gt;，&lt;br /&gt;
論文(コンピュータサイエンス系)の探し方を随分と聞かれたので，まとめてみました．  &lt;/p&gt;
&lt;p&gt;主にFeedlyでarXivみるっていう話ですけど，&lt;br /&gt;
他の情報収集元もあげてみます．&lt;/p&gt;
&lt;h2 id="feedly"&gt;Feedly&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://feedly.com"&gt;Feedly&lt;/a&gt;は，RSSリーダです．&lt;br /&gt;
もちろん，論文サーベイ以外にもRSSリーダとして使えるので，おすすめ．&lt;/p&gt;
&lt;p&gt;（&lt;a href="http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Fdaikishimada.github.io%2Ffeeds%2Fall.atom.xml"&gt;ホームにあるFollowボタンは，Feedlyのこのブログの購読ボタンだったりする&lt;/a&gt;）&lt;/p&gt;
&lt;h2 id="arxiv"&gt;arXiv&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/"&gt;arXiv&lt;/a&gt;（アーカイヴ、archiveと同じ発音）は、物理学、数学、計算機科学、量的生物学、Quantitative Finance, 統計学の、プレプリントを含む様々な論文が保存・公開されているウェブサイトである。論文のアップロード（投稿）、ダウンロード（閲覧）ともに無料で、論文はPDF形式である。1991年にスタートして、プレプリント・サーバーの先駆けとなったウェブサイトである。大文字の X をギリシャ文字のカイ（Χ）にかけて archive と読ませている。
&lt;a href="https://ja.wikipedia.org/wiki/ArXiv"&gt;arXiv - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;私がよくチェックする機械学習やコンピュータビジョン界隈では，arXivの存在はかなり知られていて，大きな国際会議に投稿されたものがアップロードされていたりします．&lt;br /&gt;
webに論文をあげるので，後から修正や補足が出来る様になっていることも，良いポイントだと思います．&lt;/p&gt;
&lt;h2 id="feedlyarxiv"&gt;FeedlyでarXivをトラッキングする&lt;/h2&gt;
&lt;p&gt;簡単にFeedlyでarXivの好きなサブジェクトを追いかける方法をまとめます．&lt;br /&gt;
（あらかじめFeedlyの会員登録がされている体で話を進めます）&lt;/p&gt;
&lt;p&gt;例として，&lt;a href="http://arxiv.org/corr/home"&gt;Computing Research Repository&lt;/a&gt;の&lt;a href="http://arxiv.org/list/cs.AI/recent"&gt;Artificial Intelligence (cs.AI)&lt;/a&gt;をFeedlyに登録してみます．&lt;br /&gt;
（CoRRのサブジェクトは全部で&lt;a href="http://arxiv.org/corr/subjectclasses"&gt;こんなに&lt;/a&gt;ある）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;arXivにアクセス&lt;br /&gt;
&lt;img alt="arXivにアクセス" src="https://www.evernote.com/l/AQCtPOQuqwRBLbFghoivStlyfAIGojywMbwB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/corr/home"&gt;Computing Research Repository&lt;/a&gt;の&lt;a href="http://arxiv.org/list/cs.AI/recent"&gt;Artificial Intelligence (cs.AI)&lt;/a&gt;へアクセス&lt;br /&gt;
&lt;img alt="csAIへアクセス" src="https://www.evernote.com/l/AQBo0xAaPi1MJZNLAeLr5gPGwZaM6ny6A8oB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;URLをコピーして置く&lt;br /&gt;
&lt;img alt="URLをコピーして置く" src="https://www.evernote.com/l/AQAePkFajEJCKKP45CYFEolVWZNbT-L5EI8B/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feedlyを開いて Add Contentをクリック&lt;br /&gt;
&lt;img alt="FeedlyのAddContentをクリック" src="https://www.evernote.com/l/AQB7CvAm-rBI4pGAEn2QlWIWhFgdHg2GMj0B/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;検索窓に購読したいページのURLを入力&lt;br /&gt;
&lt;img alt="検索窓に購読したいページのURLを入力" src="https://www.evernote.com/l/AQAecTpbRY9AeJw5dqa6TYFvxr6Mier_K0oB/image.png" /&gt;&lt;br /&gt;
&lt;img alt="入力ごEnter" src="https://www.evernote.com/l/AQANGZ23_GlH1LDX3OA7RazlEkyeet2OvTcB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;+を押してFeedlyへの登録画面へ
&lt;img alt="+を押してFeedlyへの登録画面へ" src="https://www.evernote.com/l/AQBuUm7gaL5GF7Lo6EGKmTy6TT7Tq_v9E0EB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;追加するコレクションを選んでAddを推す
&lt;img alt="追加するコレクションを選んでAddを推す" src="https://www.evernote.com/l/AQCjCjJ9cMVIPZNTSQhhFQrT47iy8ZgI8yIB/image.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これでFeedlyで論文がトラックできるようになっているはず．&lt;br /&gt;
他のサブジェクトを登録したい場合はさっきまでの作業を繰り返せばOKです．&lt;/p&gt;
&lt;p&gt;こんな感じ．
もし，"by ~"の左に数字が出ているときはそれは「&lt;a href="https://feedly.uservoice.com/knowledgebase/articles/178821-what-does-the-number-of-green-hearts-mean"&gt;engagement&lt;/a&gt;」を示す数字です．&lt;br /&gt;
その論文がどれくらいバズっているかわかるので，私がFeedlyを使う理由の一つでもあったりします．
&lt;img alt="こんな感じ" src="https://www.evernote.com/l/AQAwGNBx0oVHWrU_v4-eQ2LuMCDmXP38E7EB/image.png" /&gt;&lt;/p&gt;
&lt;p&gt;記事をクリックすれば，アブストラクトも読める！&lt;br /&gt;
&lt;img alt="記事をクリックすればアブストラクトも読める" src="https://www.evernote.com/l/AQCXJIngh0lN5KqsxuGDjGuzYhXtYbzVa9gB/image.png" /&gt;&lt;/p&gt;
&lt;p&gt;"Visit Website" をクリックすれば，arXivの該当ページに飛んで，pdfやbibtexのファイルを入手出来ます．やったね！
&lt;img alt="VisitWebsite" src="https://www.evernote.com/l/AQBd3Vae5WhKRK9t0TTr4rVGNwDa5VHMZp0B/image.png" /&gt;&lt;/p&gt;
&lt;h2 id="reddit"&gt;Redditで技術系のスレッドを見つける&lt;/h2&gt;
&lt;p&gt;arXivの話だけだと寂しいので，他の情報収集元を言うと，&lt;br /&gt;
Redditの機械学習系の議論はたまに目を通します．&lt;/p&gt;
&lt;p&gt;なかなか面白い議論のときもあれば，過疎ってるときもあるけど…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;reddit（レディット）は英語圏のウェブサイト。ニュース記事、画像のリンクやテキストを投稿し、コメントをつけることが可能。ウェブサイトへのリンクを収集・公開するソーシャルブックマークサイト。ニュース記事、画像などの紹介や感想募集のトピックを誰でも立てられるソーシャルニュースサイトでもあり、トピックについてのコメントを誰でも書き込める電子掲示板の一種でもある。 &lt;a href="https://ja.wikipedia.org/wiki/Reddit"&gt;reddit - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://twitter.com/mxlearn"&gt;ML@REDDIT&lt;/a&gt;をフォローすると，&lt;br /&gt;
機械学習系のスレッドがどんどん流れてくるので，チェックしています．&lt;/p&gt;</summary><category term="paper"></category><category term="研究"></category><category term="web"></category><category term="AI"></category></entry><entry><title>Deep Learning のトレンドについて喋ってきた</title><link href="http://daikishimada.github.io/wbaflcnn.html" rel="alternate"></link><published>2016-02-18T17:00:00+09:00</published><author><name>DaikiShimada</name></author><id>tag:daikishimada.github.io,2016-02-18:wbaflcnn.html</id><summary type="html">&lt;p&gt;Convolutional Neural NetworksのトレンドについてCasualじゃない話をしてきました．&lt;/p&gt;
&lt;h2 id="_1"&gt;全脳アーキテクチャ若手の会カジュアルトーク&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://wbawakate.jp/posts/events/12th/"&gt;全脳アーキテクチャ若手の会カジュアルトーク&lt;/a&gt;
というところでお話をしてきました．&lt;br /&gt;
ちょっと層がわからなかったのですが，IT系のエンジニアの方が多かったみたいです．&lt;br /&gt;
（学生は4人くらい…？しかもほぼ身内）&lt;/p&gt;
&lt;p&gt;僕のスライドはSlide Shareの方にアップロードされています．&lt;br /&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/NIXhrnmgjyU0xK" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="//www.slideshare.net/sheemap/convolutional-neural-networks-wbafl2" title="Convolutional Neural Networks のトレンド @WBAFLカジュアルトーク#2" target="_blank"&gt;Convolutional Neural Networks のトレンド @WBAFLカジュアルトーク#2&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a target="_blank" href="//www.slideshare.net/sheemap"&gt;Daiki Shimada&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;しかも，映像もアップロードされていた…  &lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/cBog5o9ebcQ" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2 id="_2"&gt;発表後記&lt;/h2&gt;
&lt;p&gt;実際はConvolutional Neural Networks(CNN)系論文128本ノックにするつもりだったとはいえない空気でしたね…&lt;/p&gt;
&lt;p&gt;個人的には画像生成やキャプション生成系の研究速度はとても速く進んでいると感じています．  &lt;br /&gt;
Visual Turing Test の話はもう少し掘り下げたかったですね．    &lt;/p&gt;
&lt;p&gt;Deep Mind が DQNで３D一人称視点ゲームを遊ぶ話がちょうど発表直前に出ていて，alphaGoの話題とともに紹介しましたが，あの領域は完全にGoogleの独壇場かなと思います．&lt;/p&gt;
&lt;p&gt;本当にここ最近はすごいペースで研究が進んでいくので，どこかで一回整理しないといけないなっていう危機感があり，自分なりにまとめました．  &lt;br /&gt;
サーベイってけっこう上手くやらないと労力使うし，大事な研究を見落とすこともあるので，こういう資料がだれかのサーベイの種なればいいなあ，なんて．  &lt;br /&gt;
（各々，独自にまとめていって，こういう資料がどんどん増えることを願う）&lt;/p&gt;
&lt;h2 id="_3"&gt;紹介文献一覧&lt;/h2&gt;
&lt;h3 id="cnn"&gt;CNNアーキテクチャの変遷 / 最適化手法&lt;/h3&gt;
&lt;h4 id="cnn_1"&gt;CNNアーキテクチャ&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Neocognitron&lt;ul&gt;
&lt;li&gt;K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics 36, 1980.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LeNet&lt;ul&gt;
&lt;li&gt;Y LeCun, L Bottou, Y Bengio, P Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 1998.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ave./Max Pooling, Local Contrast Normalization&lt;ul&gt;
&lt;li&gt;K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun. What is the best multi-stage architecture for object recognition?. CVPR, 2009.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ReLU&lt;ul&gt;
&lt;li&gt;X. Glorot, A. Bordes, Y. Bengio. Deep Sparse Rectifier Neural Networks. AISTATS 11, 2011.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dropout&lt;ul&gt;
&lt;li&gt;G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov. &lt;a href="http://arxiv.org/abs/1207.0580"&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt;. arXiv: 1207.0580, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AlexNet&lt;ul&gt;
&lt;li&gt;A. Krizhevsky, I. Sutskever, G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. NIPS, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Network in Network, global ave. pooling&lt;ul&gt;
&lt;li&gt;M. Lin, Q. Chen, S. Yan. &lt;a href="http://arxiv.org/abs/1312.4400"&gt;Network In Network&lt;/a&gt;. arXiv: 1312.4400, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VGG-Net&lt;ul&gt;
&lt;li&gt;K. Simonyan, A. Zisserman. &lt;a href="http://arxiv.org/abs/1409.1556"&gt;Very Deep Convolutional Networks for Large-Scale Visual Recognition&lt;/a&gt;. arXiv: 1409.1556, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GoogLeNet / Inception&lt;ul&gt;
&lt;li&gt;C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich. &lt;a href="http://arxiv.org/abs/1409.4842"&gt;Going deeper with convolutions&lt;/a&gt;. arXiv: 1409.4842, 2014.&lt;/li&gt;
&lt;li&gt;C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna. &lt;a href="http://arxiv.org/abs/1512.00567"&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;. arXiv: 1512.00567, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SPP-Net&lt;ul&gt;
&lt;li&gt;K. He, X. Zhang, S. Ren, J. Sun. &lt;a href="http://arxiv.org/abs/1406.4729"&gt;Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;. arXiv: 1406.4729, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All Convolutional Net, guided BackPropagation&lt;ul&gt;
&lt;li&gt;J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller. &lt;a href="http://arxiv.org/abs/1412.6806"&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt;. arXiv: 1412.6806, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exemplar CNN&lt;ul&gt;
&lt;li&gt;A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, T. Brox. &lt;a href="http://arxiv.org/abs/1406.6909"&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;. arXiv: 1406.6909, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Triplet network&lt;ul&gt;
&lt;li&gt;E. Hoffer, N. Ailon. &lt;a href="http://arxiv.org/abs/1412.6622"&gt;Deep metric learning using Triplet network&lt;/a&gt;. arXiv: 1412.6622, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Batch Normalization&lt;ul&gt;
&lt;li&gt;S. Ioffe, C. Szegedy. &lt;a href="http://arxiv.org/abs/1502.03167"&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;. arXiv: 1502.03167, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Residual Network; ResNet&lt;ul&gt;
&lt;li&gt;K. He, X. Zhang, S. Ren, J. Sun. &lt;a href="http://arxiv.org/abs/1512.03385"&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;. arXiv: 1512.03385, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="_4"&gt;確率的勾配降下法における学習率調整法&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AdaGrad&lt;ul&gt;
&lt;li&gt;J. Duchi, E. Hazan, Y. Singer. &lt;a href="http://stanford.edu/~jduchi/projects/DuchiHaSi10_colt.pdf"&gt;Adaptive Subgradient Methods for Online Learning and Stochastic Optimization&lt;/a&gt;. Journal of Machine Learning Research 12 ,2011.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RMSProp&lt;ul&gt;
&lt;li&gt;T. Tieleman, G. Hinton. Divide the gradient by a running average of its recent magnitude. &lt;a href="https://www.coursera.org/course/neuralnets"&gt;COURSERA: Neural Networks for Machine Learning 4&lt;/a&gt;, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AdaDelta&lt;ul&gt;
&lt;li&gt;M. D. Zeiler. &lt;a href="http://arxiv.org/abs/1212.5701"&gt;ADADELTA: An Adaptive Learning Rate Method&lt;/a&gt;. arXiv: 1212.5701, 2012.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adam&lt;ul&gt;
&lt;li&gt;D. Kingma, J. Ba. &lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. arXiv: 1412.6980, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_5"&gt;特徴量の解析 / 可視化&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeconvNet for visualizing&lt;ul&gt;
&lt;li&gt;M.D. Zeiler, and R. Fergus. &lt;a href="http://arxiv.org/abs/1311.2901"&gt;Visualizing and understanding convolutional networks&lt;/a&gt;. arXiv,: 1311.2901, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;入力画像の最適化&lt;ul&gt;
&lt;li&gt;A. Mahendran, A. Vedaldi. &lt;a href="http://arxiv.org/abs/1412.0035"&gt;Understanding Deep Image Representations by Inverting Them&lt;/a&gt;. arXiv: 1412.0035, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CNNをだます&lt;ul&gt;
&lt;li&gt;C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, R. Fergus. &lt;a href="http://arxiv.org/abs/1312.6199"&gt;Intriguing properties of neural networks&lt;/a&gt;. arXiv: 1312.6199, 2013.&lt;/li&gt;
&lt;li&gt;I. J. Goodfellow, J. Shlens, C. Szegedy. &lt;a href="http://arxiv.org/abs/1412.6572"&gt;Explaining and Harnessing Adversarial Examples&lt;/a&gt;. arXiv: 1412.6572, 2014.&lt;/li&gt;
&lt;li&gt;A. Nguyen, J. Yosinski, J. Clune. &lt;a href="http://arxiv.org/abs/1412.1897"&gt;Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images&lt;/a&gt;. arXiv: 1412.1897, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_6"&gt;物体検出・領域分割&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;R-CNN&lt;ul&gt;
&lt;li&gt;R. Girshick, J. Donahue, T. Darrell, J. Malik. &lt;a href="http://arxiv.org/abs/1311.2524"&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;. arXiv:1311.2524, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fast R-CNN&lt;ul&gt;
&lt;li&gt;R. Girshick. &lt;a href="http://arxiv.org/abs/1504.08083"&gt;Fast R-CNN&lt;/a&gt;. arXiv:1504.08083, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Faster R-CNN&lt;ul&gt;
&lt;li&gt;S. Ren, K. He, R. Girshick, J. Sun. &lt;a href="http://arxiv.org/abs/1506.01497"&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;. arXiv:1506.01497, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fully Convolutional Networks (FCN)&lt;ul&gt;
&lt;li&gt;J. Long, E. Shelhamer, T. Darrell. &lt;a href="http://arxiv.org/abs/1411.4038"&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;. arXiv: 1411.4038, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SegNet&lt;ul&gt;
&lt;li&gt;V. Badrinarayanan, A. Handa, R. Cipolla. &lt;a href="http://arxiv.org/abs/1505.07293"&gt;SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling&lt;/a&gt;. arXiv: 1505.07293, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CNN + 条件付き確率場(CRF)&lt;ul&gt;
&lt;li&gt;] S. Zheng, S. Jayasumana, B. R. Paredes, V. Vineet, Z. Su, D. Du, C. Huang, P. H. S. Torr. &lt;a href="http://arxiv.org/abs/1502.03240"&gt;Conditional Random Fields as Recurrent Neural Networks&lt;/a&gt;. arXiv: 1502.03240, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep Mask&lt;ul&gt;
&lt;li&gt;P. O. Pinheiro, R. Collobert, P. Dollar. &lt;a href="http://arxiv.org/abs/1506.06204"&gt;Learning to Segment Object Candidates&lt;/a&gt;. arXiv: 1506.06204, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep Face&lt;ul&gt;
&lt;li&gt;Y. Taigman, M. Yang, M. A. Ranzato and L. Wolf. DeepFace: Closing the Gap to Human-Level Performance in Face Verification. CVPR, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spatial Transformer Networks&lt;ul&gt;
&lt;li&gt;M. Jaderberg, K. Simonyan, A. Zisserman, K. Kavukcuoglu. &lt;a href="http://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Networks&lt;/a&gt;. arXiv: 1506.02025, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_7"&gt;画像生成・超解像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep Dream&lt;ul&gt;
&lt;li&gt;&lt;a href="http://googleresearch.blogspot.jp/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Inceptionism: Going Deeper into Neural Networks&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;K. Simonyan, A. Vedaldi, A. Zisserman. &lt;a href="http://arxiv.org/abs/1312.6034"&gt;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps&lt;/a&gt;. arXiv: 1312.6034, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モーフィング&lt;ul&gt;
&lt;li&gt;A. Dosovitskiy, J. T. Springenberg, M. Tatarchenko, T. Brox. &lt;a href="http://arxiv.org/abs/1411.5928"&gt;Learning to Generate Chairs, Tables and Cars with Convolutional Networks&lt;/a&gt;. arXiv: 1411.5928, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画風変換&lt;ul&gt;
&lt;li&gt;L. A. Gatys, A. S. Ecker, M. Bethge. &lt;a href="http://arxiv.org/abs/1508.06576"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;. arXiv: 1508.06576, 2015.&lt;/li&gt;
&lt;li&gt;C. Li, M. Wand. &lt;a href="http://arxiv.org/abs/1601.04589"&gt;Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis&lt;/a&gt;. arXiv:1601.04589, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像生成とベクトル演算性&lt;ul&gt;
&lt;li&gt;A. Radford, L. Metz, S. Chintala. &lt;a href="http://arxiv.org/abs/1511.06434"&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt;. arXiv:1511.06434, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;超解像&lt;ul&gt;
&lt;li&gt;C. Dong, C. C. Loy, K. He, X. Tang. &lt;a href="http://arxiv.org/abs/1501.00092"&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/a&gt;. arXiv:1501.00092, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://waifu2x.udp.jp/index.ja.html"&gt;waifu2x&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deblurring (モーションブラー除去)&lt;ul&gt;
&lt;li&gt;J. Sun, W. Cao, Z. Xu, J. Ponce. &lt;a href="http://arxiv.org/abs/1503.00593"&gt;Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal&lt;/a&gt;. arXiv:1503.00593, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自動彩色&lt;ul&gt;
&lt;li&gt;&lt;a href="http://tinyclouds.org/colorize/"&gt;Automatic Colorization&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;B. Hariharan, P. Arbeláez, R. Girshick, J. Malik. &lt;a href="http://arxiv.org/abs/1411.5752"&gt;Hypercolumns for Object Segmentation and Fine-grained Localization&lt;/a&gt;. arXiv: 1411.5752, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3d"&gt;3Dタスクへ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep Stereo&lt;ul&gt;
&lt;li&gt;J. Flynn, I. Neulander, J. Philbin, N. Snavely. &lt;a href="http://arxiv.org/abs/1506.06825"&gt;DeepStereo: Learning to Predict New Views from the World's Imagery&lt;/a&gt;. arXiv:1506.06825, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=cizgVZ8rjKA"&gt;DeepStereo: Learning to Predict New Views from the World’s Imagery - YouTube&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ステレオマッチング&lt;ul&gt;
&lt;li&gt;J. Žbontar, Y. LeCun. &lt;a href="http://arxiv.org/abs/1510.05970"&gt;Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches&lt;/a&gt;. arXiv: 1510.05970, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;単一画像による3Dタスク&lt;ul&gt;
&lt;li&gt;D. Eigen, R. Fergus. &lt;a href="http://arxiv.org/abs/1411.4734"&gt;Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture&lt;/a&gt;. arXiv: 1411.4734, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_8"&gt;映像への挑戦&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;スポーツ映像分類&lt;ul&gt;
&lt;li&gt;A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, F. Li. Large-scale Video Classification with Convolutional Neural Networks. CVPR, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_9"&gt;より “人間らしい” 機械知覚へ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MemNet: CNN for Memorability&lt;ul&gt;
&lt;li&gt;&lt;a href="http://memorability.csail.mit.edu/index.html"&gt;LaMem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A. Khosla, A. S. Raju, A. Torralba and A. Oliva. Understanding and Predicting Image Memorability at a Large Scale. ICCV, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_10"&gt;マルチモーダル・アプリケーション&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;キャプション生成&lt;ul&gt;
&lt;li&gt;O. Vinyals, A. Toshev, S. Bengio, D. Erhan. Show and Tell: A Neural Image Caption Generator. arXiv: 1411.4555, 2014.&lt;/li&gt;
&lt;li&gt;J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell. &lt;a href="http://arxiv.org/abs/1411.4389"&gt;Long-term Recurrent Convolutional Networks for Visual Recognition and Description&lt;/a&gt;. arXiv: 1411.4389, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像に関する質問文に答える&lt;ul&gt;
&lt;li&gt;H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, W. Xu. &lt;a href="http://arxiv.org/abs/1505.05612"&gt;Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering&lt;/a&gt;. arXiv: 1505.05612, 2015.&lt;/li&gt;
&lt;li&gt;M. Malinowski, M. Rohrbach, M. Fritz. Ask Your Neurons: A Neural-Based Approach to Answering Questions About Images. ICCV, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;マルチモーダルな情報表現&lt;ul&gt;
&lt;li&gt;R. Kiros, R. Salakhutdinov, R. S. Zemel. &lt;a href="http://arxiv.org/abs/1411.2539"&gt;Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models&lt;/a&gt;. arXiv: 1411.2539, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cnn_2"&gt;CNNと強化学習&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Playing Atari&lt;ul&gt;
&lt;li&gt;V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller. &lt;a href="http://arxiv.org/abs/1312.5602"&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt;. arXiv:1312.5602, 2013.&lt;/li&gt;
&lt;li&gt;V. Mnih, at al. Human-level control through deep reinforcement learning. nature, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AlphaGo&lt;ul&gt;
&lt;li&gt;D. Silver, et al. Mastering the game of Go with deep neural networks and tree search. nature, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Facebookによる囲碁AI&lt;ul&gt;
&lt;li&gt;Y. Tian, Y. Zhu. &lt;a href="http://arxiv.org/abs/1511.06410"&gt;Better Computer Go Player with Neural Network and Long-term Prediction&lt;/a&gt;. arXiv: 1511.06410, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一人称3Dゲーム&lt;ul&gt;
&lt;li&gt;V. Mnih, A.P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu. &lt;a href="http://arxiv.org/abs/1602.01783"&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;. arXiv:1602.01783, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="whats-next"&gt;What’s Next ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Visual Genome&lt;ul&gt;
&lt;li&gt;&lt;a href="https://visualgenome.org/"&gt;Visual Genome&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="deeplearning"></category></entry></feed>